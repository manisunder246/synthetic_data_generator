{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b1c83e",
   "metadata": {},
   "source": [
    "## Version-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b811ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streamlit_app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import base64\n",
    "import re\n",
    "import pingouin as pg\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Set page config with page title and layout\n",
    "st.set_page_config(page_title='DataWeave', layout='wide')\n",
    "\n",
    "# Adjust the layout to move the title up\n",
    "col_logo, col_title = st.columns([1, 3])\n",
    "\n",
    "with col_logo:\n",
    "    st.image('/Users/maniveena/Desktop/Gen_AI/Synthetic DATA/logo.png', width=200)  # Replace with the path to your logo image\n",
    "\n",
    "with col_title:\n",
    "    st.markdown(\"<h1 style='text-align: center;'>DataWeave - Synthetic Data Generator</h1>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<p style='text-align: center;'>Generate synthetic data based on your custom requirements with AI.</p>\", unsafe_allow_html=True)\n",
    "\n",
    "# Initialize session state variables if they don't exist\n",
    "if 'field_names' not in st.session_state:\n",
    "    st.session_state['field_names'] = []\n",
    "if 'field_types' not in st.session_state:\n",
    "    st.session_state['field_types'] = []\n",
    "\n",
    "options = [\n",
    "    'Text', 'integer', 'Float', 'Date', 'Boolean', 'Enum', 'Email',\n",
    "    'IPAddress', 'URL', 'PhoneNumber', 'Currency', 'TextBlob', 'Name',\n",
    "    'Country', 'City', 'State', 'ZipCode', 'Latitude', 'Longitude'\n",
    "]\n",
    "\n",
    "domains = [\n",
    "    'Medicine', 'Finance', 'E-commerce', 'Logistics', 'Travel',\n",
    "    'Social Media', 'Education', 'Real Estate', 'Gaming',\n",
    "    'Automotive', 'Energy', 'Technology', 'Retail', 'Entertainment',\n",
    "    'Telecommunications'\n",
    "]\n",
    "\n",
    "# Define add_field function to add a new field to the session state\n",
    "def add_field():\n",
    "    st.session_state['field_names'].append('')\n",
    "    st.session_state['field_types'].append('')\n",
    "\n",
    "# Custom CSS to create boxes around sections\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "    .box {\n",
    "        border: none;\n",
    "        border-radius: 5px;\n",
    "        padding: 10px;\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "# DEFINING ALL THE RELEVANT FUNCTIONS\n",
    "\n",
    "# Function to capture and validate user inputs\n",
    "def capture_and_validate_inputs():\n",
    "    # Check if the necessary inputs are present in the session state\n",
    "    if 'field_names' in st.session_state and 'field_types' in st.session_state:\n",
    "        # Capture the field names and types\n",
    "        field_names = st.session_state['field_names']\n",
    "        field_types = st.session_state['field_types']\n",
    "\n",
    "        # Capture the domain and description if they are provided\n",
    "        domain = st.session_state.get('domain_info', 'Not Specified')\n",
    "        description = st.session_state.get('description_info', 'Not Specified')\n",
    "\n",
    "        # Print the captured inputs for validation\n",
    "        print(\"Captured User Inputs:\")\n",
    "        print(\"Field Names:\", field_names)\n",
    "        print(\"Field Types:\", field_types)\n",
    "        print(\"Domain:\", domain)\n",
    "        print(\"Description:\", description)\n",
    "\n",
    "        return field_names, field_types, domain, description\n",
    "    else:\n",
    "        print(\"Required inputs are not fully provided.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "# Function to capture the input and display it on the website\n",
    "def display_formatted_data(field_names, field_types, domain, description):\n",
    "    st.subheader(\"Captured Data for Validation\")\n",
    "\n",
    "    # Using columns to control the width of the display\n",
    "    col1, col2, col3 = st.columns([3, 1, 1])  # Adjust column ratios as needed\n",
    "\n",
    "    with col1:\n",
    "        st.markdown(\"**Field Names and Types**\")\n",
    "        if field_names and field_types:\n",
    "            data_df = pd.DataFrame({'Field Name': field_names, 'Field Type': field_types})\n",
    "            st.dataframe(data_df.style.set_table_styles([{'selector': 'th', 'props': [('max-width', '200px')]}]), width=500)  # Adjust width as needed\n",
    "        else:\n",
    "            st.write(\"No field data captured.\")\n",
    "\n",
    "    with col2:\n",
    "        st.markdown(\"**Domain**\")\n",
    "        st.write(domain)\n",
    "\n",
    "    with col3:\n",
    "        st.markdown(\"**Description**\")\n",
    "        st.write(description)\n",
    "        \n",
    "# Function to construct the prompt for the LLM\n",
    "def construct_prompt(field_names, field_types, domain, description):\n",
    "    # Specify the tabular format in the prompt\n",
    "    prompt = f\"Create a table of data in the domain of {domain}. \"\n",
    "    prompt += f\"The table should have the following columns: {', '.join(field_names)}. \"\n",
    "    prompt += f\"Each column should follow the type specified: {', '.join([f'{name} ({f_type})' for name, f_type in zip(field_names, field_types)])}. \"\n",
    "    prompt += f\"Description: {description}. \"\n",
    "    prompt += \"Format the data as a table with each row representing a unique entry.\\n\\n\"\n",
    "   # Add your custom message with rules\n",
    "    prompt += \"This is are STRICT RULES which you should never violate:\\n\"\n",
    "    prompt += \"1. Only generate tabular data AND make sure you generate data based on the number of rows the USER SPECIFIED. Not more data or less data.\\n\"\n",
    "    prompt += \"2. DO NOT add any TEXT whatsoever apart from the data you generate. for ex \\\"Note: This table shows only a few sample entries. You can add more rows to complete the dataset.\\\"\\n\"\n",
    "    prompt += \"3. REMEMBER the data you generated will be converted to TXT, JSON, or CSV as per the user requirement. So Any unwanted data will cause unnecessary data conversion problems. YOU SHOULD ENSURE your data COMPLIES to the above 2 rules.\"\n",
    "    prompt += \"4. When you generate personal data like Email, IPAddress, URL, PhoneNumber, Zip Code etc. ENSURE THAT they are FAKE i.e. by giving made up domains, URLs etc.\"\n",
    "    prompt += \"5. THE MOST IMPORTANT RULE: Format the data as a table with columns separated by '|' and rows separated by new lines.\\n\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to call OpenAI API to generate synthetic data\n",
    "\n",
    "OPENAI_API_KEY = 'your-api-key-xxxxxxxxx-xxx234356%%&*^'  #Set your OpenAI API key here\n",
    "#client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "async def generate_synthetic_data(prompt):\n",
    "    try:\n",
    "        # Ensure the prompt is not empty\n",
    "        if not prompt:\n",
    "            raise ValueError(\"Prompt is empty\")\n",
    "\n",
    "        # Call the OpenAI API using the new method for chat completions\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        # Assuming the response structure matches the chat completion format\n",
    "        # The actual response content might be nested differently\n",
    "        # You will need to adjust the following line based on the actual structure\n",
    "        # This is an example based on common structures for chat responses\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while generating data: {e}\"\n",
    "\n",
    "# Function to parse the output of LLM\n",
    "\n",
    "\n",
    "def parse_llm_output(output):\n",
    "    # Split the output into lines\n",
    "    lines = output.strip().split('\\n')\n",
    "    \n",
    "    # Remove leading and trailing '|' characters and strip spaces for each line\n",
    "    lines = [line.strip('|').strip() for line in lines]\n",
    "\n",
    "    # Process the header line\n",
    "    headers = [header.strip() for header in lines[0].split('|')]\n",
    "\n",
    "    # Process the data lines\n",
    "    data = []\n",
    "    for line in lines[2:]:  # Skip the dashed separator line\n",
    "        row = [element.strip() for element in line.split('|')]\n",
    "        data.append(row)\n",
    "\n",
    "    # Create a DataFrame from the processed data\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "    # Function to convert data to different formats\n",
    "def convert_data_format(data, format_type):\n",
    "    if format_type == 'CSV':\n",
    "        converted_data = data.to_csv(index=False).encode('utf-8')\n",
    "    elif format_type == 'JSON':\n",
    "        converted_data = data.to_json(orient='records').encode('utf-8')\n",
    "    elif format_type == 'Excel':\n",
    "        output = BytesIO()\n",
    "        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "            data.to_excel(writer, index=False)\n",
    "        output.seek(0)\n",
    "        converted_data = output.getvalue()\n",
    "    elif format_type == 'TXT':\n",
    "        converted_data = data.to_csv(index=False, sep='\\t').encode('utf-8')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported format type\")\n",
    "    return converted_data\n",
    "\n",
    "\n",
    "# Function to download data in selected format\n",
    "def modified_download_button(data, format_type, file_name):\n",
    "    if format_type == 'Excel':\n",
    "        mime_type = \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
    "    elif format_type == 'CSV':\n",
    "        mime_type = \"text/csv\"\n",
    "    elif format_type == 'JSON':\n",
    "        mime_type = \"application/json\"\n",
    "    elif format_type == 'TXT':\n",
    "        mime_type = \"text/plain\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported format type\")\n",
    "\n",
    "    # Check for the state to show the download button\n",
    "    show_download_key = f\"show_download_button_{format_type}\"\n",
    "    if st.session_state.get(show_download_key, False):\n",
    "        st.download_button(\n",
    "            label=f\"Download data as {format_type}\",\n",
    "            data=data,\n",
    "            file_name=file_name,\n",
    "            mime=mime_type\n",
    "        )\n",
    "def analyze_data(data):\n",
    "    try:\n",
    "        report = {}\n",
    "\n",
    "        # Descriptive statistics for numeric fields\n",
    "        numeric_cols = data.select_dtypes(include=[np.number])\n",
    "        if not numeric_cols.empty:\n",
    "            report['Descriptive Statistics for Numeric Fields'] = numeric_cols.describe()\n",
    "            # Distribution checks\n",
    "            skewness = numeric_cols.skew().to_frame(name='Skewness')\n",
    "            kurtosis = numeric_cols.kurtosis().to_frame(name='Kurtosis')\n",
    "            distribution_stats = pd.concat([skewness, kurtosis], axis=1)\n",
    "            report['Distribution Statistics for Numeric Fields'] = distribution_stats\n",
    "\n",
    "        # Descriptive statistics for categorical fields\n",
    "        categorical_cols = data.select_dtypes(include=['object', 'category'])\n",
    "        if not categorical_cols.empty:\n",
    "            report['Descriptive Statistics for Categorical Fields'] = categorical_cols.describe()\n",
    "\n",
    "        # Correlation matrix for the data\n",
    "        if len(numeric_cols.columns) > 1:\n",
    "            report['Correlation Matrix'] = numeric_cols.corr()\n",
    "\n",
    "        return report\n",
    "    except Exception as e:\n",
    "        print(\"Error in analyze_data:\", e)\n",
    "        return {\"Error\": f\"Error occurred during data analysis: {e}\"}\n",
    "    \n",
    "def calculate_icc(df, rating_col, group_col, rater_col):\n",
    "    icc = pg.intraclass_corr(data=df, targets=group_col, raters=rater_col, ratings=rating_col).round(3)\n",
    "    return icc\n",
    "\n",
    "\n",
    "def perform_factor_analysis(df, num_factors):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    if numeric_df.shape[1] < num_factors:\n",
    "        return \"Not enough numeric columns for factor analysis.\"\n",
    "\n",
    "    fa = FactorAnalyzer(n_factors=num_factors, method='principal', rotation=None)\n",
    "    fa.fit(numeric_df)\n",
    "    return pd.DataFrame(fa.loadings_, index=numeric_df.columns)\n",
    "\n",
    "def distribution_comparison(df1, df2):\n",
    "    common_columns = set(df1.columns).intersection(df2.columns)\n",
    "    numeric_columns = [col for col in common_columns if df1[col].dtype in ['float64', 'int64']]\n",
    "    results = {}\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        stat, p_value = ks_2samp(df1[column].dropna(), df2[column].dropna())\n",
    "        results[column] = {'KS Statistic': stat, 'P-Value': p_value}\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_data_with_additional_tests(original_data, synthetic_data):\n",
    "    try:\n",
    "        reports = {}\n",
    "\n",
    "        # Standard Analysis\n",
    "        reports['Original Data Analysis'] = analyze_data(original_data)\n",
    "        reports['Synthetic Data Analysis'] = analyze_data(synthetic_data)\n",
    "\n",
    "        # Intraclass Correlation Coefficient (ICC)\n",
    "        # Assuming the data has the required structure for ICC\n",
    "        #reports['ICC Analysis'] = calculate_icc(df=original_data, rating_col='rating', group_col='group', rater_col='rater')\n",
    "\n",
    "        # Factor Analysis\n",
    "        # Perform only if the original data is suitable for factor analysis\n",
    "        reports['Factor Analysis'] = perform_factor_analysis(df=original_data, num_factors=5)\n",
    "\n",
    "        # Distribution Comparisons\n",
    "        reports['Distribution Comparison'] = distribution_comparison(original_data, synthetic_data)\n",
    "\n",
    "        return reports\n",
    "    except Exception as e:\n",
    "        print(\"Error in combined analysis:\", e)\n",
    "        return {\"Error\": f\"Error occurred during combined data analysis: {e}\"}\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "type_mapping = {\n",
    "    'text': 'object',\n",
    "    'integer': 'int64',\n",
    "    'float': 'float64',\n",
    "    'date': 'datetime64',\n",
    "    'boolean': 'bool',\n",
    "    'enum': 'category',\n",
    "    'email': 'object',\n",
    "    'ipaddress': 'object',\n",
    "    'url': 'object',\n",
    "    'phonenumber': 'object',\n",
    "    'currency': 'float64',  # Assuming currency values are to be treated as numerical values\n",
    "    'textblob': 'object',\n",
    "    'name': 'object',\n",
    "    'country': 'object',\n",
    "    'city': 'object',\n",
    "    'state': 'object',\n",
    "    'zipcode': 'object',\n",
    "    'latitude': 'float64',\n",
    "    'longitude': 'float64'\n",
    "}\n",
    "\n",
    "\n",
    "def standardize_and_convert_data_types(df, field_names, field_types):\n",
    "    for field_name, user_type in zip(field_names, field_types):\n",
    "        standardized_type = type_mapping.get(user_type.lower().replace(\" \", \"\"), None)\n",
    "        if standardized_type:\n",
    "            try:\n",
    "                if standardized_type == 'int64':\n",
    "                    df[field_name] = pd.to_numeric(df[field_name], errors='coerce').astype('Int64')\n",
    "                elif standardized_type == 'float64':\n",
    "                    df[field_name] = pd.to_numeric(df[field_name], errors='coerce').astype('float')\n",
    "                elif standardized_type == 'datetime64':\n",
    "                    df[field_name] = pd.to_datetime(df[field_name], errors='coerce')\n",
    "                elif standardized_type == 'bool':\n",
    "                    df[field_name] = df[field_name].astype('bool')\n",
    "                # Additional data types can be added here.\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {field_name} to {standardized_type}: {e}\")\n",
    "        else:\n",
    "            print(f\"No mapping found for type '{user_type}', field '{field_name}' will remain unchanged.\")\n",
    "    return df\n",
    "\n",
    "def infer_field_details_from_dataset(df):\n",
    "    # Expand the type mapping to include more data types\n",
    "    type_mapping = {\n",
    "        'int64': 'integer',\n",
    "        'int32': 'integer',\n",
    "        'int16': 'integer',\n",
    "        'int8': 'integer',\n",
    "        'float64': 'float',\n",
    "        'float32': 'float',\n",
    "        'float16': 'float',\n",
    "        'object': 'text',\n",
    "        'bool': 'boolean',\n",
    "        'datetime64[ns]': 'date',\n",
    "        'datetime64[ns, tz]': 'date',\n",
    "        'timedelta[ns]': 'text',  # timedelta is not always directly supported\n",
    "        'category': 'enum',\n",
    "        'string': 'text',\n",
    "        # Add other pandas dtypes if needed\n",
    "    }\n",
    "\n",
    "    field_names = df.columns.tolist()\n",
    "    field_types = df.dtypes.apply(lambda x: type_mapping.get(str(x), 'text')).tolist()\n",
    "    \n",
    "    return field_names, field_types\n",
    "\n",
    "\n",
    "\n",
    "# Create two blocks for Manual Data Specification and Upload CSV File\n",
    "col_manual_data_spec, col_csv_upload = st.columns([3, 2])\n",
    "\n",
    "with col_manual_data_spec:\n",
    "    st.markdown(\"<div class='box'>\", unsafe_allow_html=True)\n",
    "    st.subheader('Manual Data Specification')\n",
    "\n",
    "    if st.button('Add Field'):\n",
    "        add_field()\n",
    "    \n",
    "    with st.form(key='manual_data_form'):\n",
    "        all_fields_filled = True\n",
    "        for i in range(len(st.session_state['field_names'])):\n",
    "            col1, col2 = st.columns([3, 2])\n",
    "            with col1:\n",
    "                field_name = st.text_input(f'Field Name {i+1}', value=st.session_state['field_names'][i], key=f'field_name_{i}')\n",
    "                st.session_state['field_names'][i] = field_name\n",
    "                if not field_name:\n",
    "                    all_fields_filled = False\n",
    "            with col2:\n",
    "                field_type = st.selectbox(f'Field Type {i+1}', options, index=options.index(st.session_state['field_types'][i]) if st.session_state['field_types'][i] in options else 0, key=f'field_type_{i}')\n",
    "                st.session_state['field_types'][i] = field_type\n",
    "                if not field_type:\n",
    "                    all_fields_filled = False\n",
    "\n",
    "        domain = st.selectbox('Domain Info (Mandatory)', [''] + domains, key='domain_info')\n",
    "        description = st.text_area('Description (Mandatory)', key='description_info')\n",
    "        if not domain or not description:\n",
    "            all_fields_filled = False\n",
    "\n",
    "        submit_button = st.form_submit_button('Generate Data', disabled=not all_fields_filled)\n",
    "        \n",
    "\n",
    "csv_data = None\n",
    "with col_csv_upload:\n",
    "    st.markdown(\"<div class='box'>\", unsafe_allow_html=True)\n",
    "    st.subheader('Upload CSV File')\n",
    "\n",
    "    # Start of the form for CSV upload\n",
    "    with st.form(key='csv_upload_form'):\n",
    "        uploaded_file = st.file_uploader(\"Drag and drop CSV file here\", type=['csv'], key='uploaded_file')\n",
    "        domain_csv = st.selectbox('Domain Info (Mandatory)', [''] + domains, key='domain_csv_info')\n",
    "        description_csv = st.text_area('Description (Mandatory)', key='description_csv_info')\n",
    "        submit_csv_button = st.form_submit_button('Generate Data from CSV')\n",
    "\n",
    "if submit_button or (submit_csv_button and uploaded_file is not None):\n",
    "    # Manual data specification form submitted\n",
    "    if submit_button:\n",
    "        field_names, field_types, domain, description = capture_and_validate_inputs()\n",
    "\n",
    "    # CSV upload form submitted\n",
    "    if submit_csv_button and uploaded_file is not None:\n",
    "        try:\n",
    "            # Read the uploaded CSV file\n",
    "            csv_data = pd.read_csv(uploaded_file)\n",
    "            # Check if the CSV is a metadata file by looking for specific markers (e.g., a header)\n",
    "            # Here, we assume the metadata file has a column 'Field Type'\n",
    "            if 'Field Type' in csv_data.columns:\n",
    "                # Extract data from the CSV file\n",
    "                field_names = csv_data.iloc[:, 0].tolist()  # First column as field names\n",
    "                field_types = csv_data.iloc[:, 1].tolist()  # Second column as field types\n",
    "                domain = domain_csv\n",
    "                description = description_csv\n",
    "            else:\n",
    "                field_names, field_types = infer_field_details_from_dataset(csv_data)\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred while reading the CSV file: {e}\")\n",
    "        \n",
    "        # Construct the prompt and generate synthetic data\n",
    "        if field_names and field_types:\n",
    "            prompt = construct_prompt(field_names, field_types, domain, description)\n",
    "\n",
    "            synthetic_data = asyncio.run(generate_synthetic_data(prompt))\n",
    "\n",
    "            try:\n",
    "                synthetic_df = parse_llm_output(synthetic_data)\n",
    "                synthetic_df = standardize_and_convert_data_types(synthetic_df, field_names, field_types)\n",
    "\n",
    "                # Perform statistical analysis on the generated data\n",
    "                if not synthetic_df.empty:\n",
    "                    if 'Field Type' not in csv_data.columns:\n",
    "                        # Analysis for dataset uploads\n",
    "                        combined_reports = analyze_data_with_additional_tests(csv_data, synthetic_df)\n",
    "                        for title, report in combined_reports.items():\n",
    "                            st.markdown(f\"### {title}\")\n",
    "                            if isinstance(report, pd.DataFrame):\n",
    "                                st.table(report)\n",
    "                            else:\n",
    "                                st.write(report)\n",
    "                    else:\n",
    "                        # Analysis for metadata uploads\n",
    "                        report_dict = analyze_data(synthetic_df)\n",
    "                        for title, df in report_dict.items():\n",
    "                            st.markdown(f\"### {title}\")\n",
    "                            st.table(df)\n",
    "                else:\n",
    "                    st.write(\"No synthetic data available for analysis.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error parsing LLM output: {e}\")\n",
    "                synthetic_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "        # Check if synthetic data was generated and the DataFrame is not empty\n",
    "        if 'show_download_button_JSON' not in st.session_state:\n",
    "            st.session_state['show_download_button_JSON'] = False\n",
    "        if 'show_download_button_CSV' not in st.session_state:\n",
    "            st.session_state['show_download_button_CSV'] = False\n",
    "        if 'show_download_button_Excel' not in st.session_state:\n",
    "            st.session_state['show_download_button_Excel'] = False\n",
    "        if 'show_download_button_TXT' not in st.session_state:\n",
    "            st.session_state['show_download_button_TXT'] = False\n",
    "\n",
    "        # Code block where the format is selected and the download button is displayed\n",
    "        if synthetic_data and not synthetic_df.empty:\n",
    "            # Let the user choose the download format\n",
    "            download_format = st.selectbox('Select the format for download', ['CSV', 'JSON', 'Excel', 'TXT'])\n",
    "            converted_data = convert_data_format(synthetic_df, download_format)\n",
    "\n",
    "            if converted_data is not None:\n",
    "                # Set the session state to show the download button for the selected format\n",
    "                st.session_state[f'show_download_button_{download_format}'] = True\n",
    "\n",
    "                # Call the modified download button function\n",
    "                modified_download_button(converted_data, download_format, f\"synthetic_data.{download_format.lower()}\")\n",
    "            else:\n",
    "                st.error(\"Failed to convert data to the selected format.\")\n",
    "                st.session_state[f'show_download_button_{download_format}'] = False\n",
    "\n",
    "# Run this from the command line:\n",
    "# streamlit run streamlit_app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b6b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
